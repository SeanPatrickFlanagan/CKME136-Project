---
title: "CKME 136 Readmission Data Analysis"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r}
# uncomment the next lines to install required packages and load the libraries
#install.packages("Hmisc")
#install.packages("summarytools")
#install.packages("pastecs")
#install.packages("corrplot")
#install.packages("moments")
#install.packages("rcompanion")    # required for transformTukey function
#install.packages("dplyr")
#install.packages("e1071")
#install.packages("FSelector")    # required for FSelector and RWeka libraries
#install.packages("mlbench")
#install.packages("tictoc")
#install.packages("DMwR")
#install.packages("RWeka")
#install.packages("randomForest")
#install.packages("mlbench")
#install.packages("DescTools")    # g-test goodness of fit
#install.packages("arules")
#install.packages("fastAdaboost")

library("pastecs")        # required for stat.desc function for summary statistics
# library("summarytools")
library("Hmisc")          # describe function for summary statistics
library("corrplot")       # corrplot function to plot correlation graphics
# library("moments")
library("rcompanion")     # required for transformTukey function
library("plyr")
library("dplyr")          # required for %>% pipe
library("FSelector")      # feature selection algorithm
library("mlbench")
library("caret")          # used for createPartition, 
# library("Rfast")
library("arules")         # association rules ... apriori
library("e1071")          # required for naivebayes algorithm
library("RWeka")          # required for J48 decision tree algorithm
library("randomForest")   # required for randomForest algorithm
library("tictoc")         # used to measure runtime of code
library("DMwR")           # required for SMOTE
library("DescTools")      # required for G-test ... goodness of fit
library("fastAdaboost")   # required for Adaboost ensemble package
```

```{r}
readmission_data_original <- read.csv("C:/Data/Education/Ryerson/CKME136/dataset_diabetes/diabetic_data.csv")
# using a working copy of the data set to preserve original data for reference
readmission_data <-readmission_data_original
## view the first few rows of the data
head(readmission_data)
```
```{r}
data_desc <- read.csv("C:/Data/Education/Ryerson/CKME136/dataset_diabetes/IDs_mapping.csv")
# view the first few rows of the data description
data_desc
```
```{r}
# Basic summary statistics
# Make a vector of categorical (factor) variables
a <- (sapply(readmission_data,class) == "factor")

# Categorical variables (use describe function of Hmisc package to display summary)
cat_vars <- readmission_data[, a]
Hmisc::describe(cat_vars)
for (cat_var in (1:ncol(cat_vars))){
  barplot(table(cat_vars[, cat_var]), main = names(cat_vars[cat_var]), las=2)
}

# Numerical variables (use stat.desc function of pastecs package to display summary)
num_vars <- readmission_data[, !a]
# Do not include the encounter_id or the patient_nbr
num_vars <- subset(num_vars, select = -c(1,2))
options(scipen=100)
options(digits=3)
stat.desc(num_vars)
for (num_var in (1:ncol(num_vars))){
  hist(num_vars[num_var])
}

```


```{r}
# Removal of Records
# Remove if gender is "Unknown/Invalid" since there are so few records (3) in this category
readmission_data <- readmission_data[!readmission_data$gender == "Unknown/Invalid",]
# Remove unused "Unknown/Invalid" level in gender factor attribute
readmission_data$gender <- factor(readmission_data$gender)

# Remove if all three diagnoses are missing (at least one of these should be diabetes related)
readmission_data <- readmission_data[!(readmission_data$diag_1 == "?" & readmission_data$diag_2 == '?' & readmission_data$diag_3 == '?'),]

# Remove if patient died or discharged to hospice since they can not be readmitted
readmission_data <- readmission_data[!(readmission_data$discharge_disposition_id %in% c(11, 13, 14, 19, 20, 21)),]

# Retain only records for the first encounter for each patient to avoid biasing toward patients with multiple encounters
readmission_data <- readmission_data[order(readmission_data$patient_nbr, readmission_data$encounter_id),]
readmission_data <- readmission_data[!duplicated(readmission_data$patient_nbr),]

# Remove "weight" attribute due to 98% missing values
readmission_data <- subset(readmission_data, select = -c(weight))
```

```{r}
# Categorical Variables
#Recategorize diagnoses ICD-9 codes to 9 disease categories including "Other" category for infrequent codes
# ***
# Attribute diag_1
# ***
#create a working copy of diagnosis 1 attribute
readmission_data["copy_diag_1"] <- readmission_data$diag_1
#convert ICD9 codes to numeric to facilitate searching
# set non-numeric codes to zero (these will ultimately be reclassified as "Other")
levels(readmission_data$copy_diag_1)[levels(readmission_data$copy_diag_1) == "?"] <- '0'
levels(readmission_data$copy_diag_1)[substr(levels(readmission_data$copy_diag_1), 1, 1) == "V"] <- '0'
levels(readmission_data$copy_diag_1)[substr(levels(readmission_data$copy_diag_1), 1, 1) == "E"] <- '0'

#diabetes codes begin with 250 but may have decimal code after 250, so include up to 5 digits
options(digits=5)

# change diagnosis date type from factor to numeric (convert to character first, otherwise get levels of factor not the actual values)
readmission_data$copy_diag_1 <- as.numeric(as.character(readmission_data$copy_diag_1))

#add levels for 9 disease categories
levels(readmission_data$diag_1) <- c(levels(readmission_data$diag_1),"Circulatory", "Respiratory", "Digestive", "Diabetes", "Injury", "Musculoskeletal", "Genitourinary", "Neoplasms", "Other")

#set all entries for first primary diagnosis to "Other" as the default and only reassign those entries that belong to one of the other eight diagnosis categories
readmission_data$diag_1 <- "Other"

readmission_data$diag_1[((readmission_data$copy_diag_1 >= 390) & (readmission_data$copy_diag_1 <= 459)) | (readmission_data$copy_diag_1 == 785)] <- "Circulatory"

readmission_data$diag_1[((readmission_data$copy_diag_1 >= 460) & (readmission_data$copy_diag_1 <= 519)) | (readmission_data$copy_diag_1 == 786)] <- 'Respiratory'

readmission_data$diag_1[((readmission_data$copy_diag_1 >= 520) & (readmission_data$copy_diag_1 <= 579)) | (readmission_data$copy_diag_1 == 787)] <- 'Digestive'

readmission_data$diag_1[((readmission_data$copy_diag_1 >= 250) & (readmission_data$copy_diag_1 < 251))] <- 'Diabetes'

readmission_data$diag_1[((readmission_data$copy_diag_1 >= 800) & (readmission_data$copy_diag_1 <= 999))] <- 'Injury'

readmission_data$diag_1[((readmission_data$copy_diag_1 >= 710) & (readmission_data$copy_diag_1 <= 739))] <- 'Musculoskeletal'

readmission_data$diag_1[((readmission_data$copy_diag_1 >= 580) & (readmission_data$copy_diag_1 <= 629)) | (readmission_data$copy_diag_1 == 788)] <- 'Genitourinary'

readmission_data$diag_1[((readmission_data$copy_diag_1 >= 140) & (readmission_data$copy_diag_1 <= 239))] <- 'Neoplasm'


# ***
# Attribute diag_2
# ***
#create a working copy of diagnosis 2 attribute
readmission_data["copy_diag_2"] <- readmission_data$diag_2
#convert ICD9 codes to numeric to facilitate searching
# set non-numeric codes to zero (these will ultimately be reclassified as "Other")
levels(readmission_data$copy_diag_2)[levels(readmission_data$copy_diag_2) == "?"] <- '0'
levels(readmission_data$copy_diag_2)[substr(levels(readmission_data$copy_diag_2), 1, 1) == "V"] <- '0'
levels(readmission_data$copy_diag_2)[substr(levels(readmission_data$copy_diag_2), 1, 1) == "E"] <- '0'

#diabetes codes begin with 250 but may have decimal code after 250, so include up to 5 digits
options(digits=5)

# change diagnosis date type from character to numeric
readmission_data$copy_diag_2 <- as.numeric(as.character(readmission_data$copy_diag_2))

#add levels for 9 disease categories
levels(readmission_data$diag_2) <- c(levels(readmission_data$diag_2),"Circulatory", "Respiratory", "Digestive", "Diabetes", "Injury", "Musculoskeletal", "Genitourinary", "Neoplasms", "Other")

#set all entries for second primary diagnosis to "Other" as the default and only reassign those entries that belong to one of the other eight diagnosis categories
readmission_data$diag_2 <- "Other"

readmission_data$diag_2[((readmission_data$copy_diag_2 >= 390) & (readmission_data$copy_diag_2 <= 459)) | (readmission_data$copy_diag_2 == 785)] <- "Circulatory"

readmission_data$diag_2[((readmission_data$copy_diag_2 >= 460) & (readmission_data$copy_diag_2 <= 519)) | (readmission_data$copy_diag_2 == 786)] <- 'Respiratory'

readmission_data$diag_2[((readmission_data$copy_diag_2 >= 520) & (readmission_data$copy_diag_2 <= 579)) | (readmission_data$copy_diag_2 == 787)] <- 'Digestive'

readmission_data$diag_2[((readmission_data$copy_diag_2 >= 250) & (readmission_data$copy_diag_2 < 251))] <- 'Diabetes'

readmission_data$diag_2[((readmission_data$copy_diag_2 >= 800) & (readmission_data$copy_diag_2 <= 999))] <- 'Injury'

readmission_data$diag_2[((readmission_data$copy_diag_2 >= 710) & (readmission_data$copy_diag_2 <= 739))] <- 'Musculoskeletal'

readmission_data$diag_2[((readmission_data$copy_diag_2 >= 580) & (readmission_data$copy_diag_2 <= 629)) | (readmission_data$copy_diag_2 == 788)] <- 'Genitourinary'

readmission_data$diag_2[((readmission_data$copy_diag_2 >= 140) & (readmission_data$copy_diag_2 <= 239))] <- 'Neoplasm'

# ***
# Attribute diag_3
# ***
#create a working copy of diagnosis 3 attribute
readmission_data["copy_diag_3"] <- readmission_data$diag_3

#convert ICD9 codes to numeric to facilitate searching
# set non-numeric codes to zero (these will ultimately be reclassified as "Other")
levels(readmission_data$copy_diag_3)[levels(readmission_data$copy_diag_3) == "?"] <- '0'
levels(readmission_data$copy_diag_3)[substr(levels(readmission_data$copy_diag_3), 1, 1) == "V"] <- '0'
levels(readmission_data$copy_diag_3)[substr(levels(readmission_data$copy_diag_3), 1, 1) == "E"] <- '0'

#diabetes codes begin with 250 but may have decimal code after 250, so include up to 5 digits
options(digits=5)

# change diagnosis date type from character to numeric
readmission_data$copy_diag_3 <- as.numeric(as.character(readmission_data$copy_diag_3))

#add levels for 9 disease categories
levels(readmission_data$diag_3) <- c(levels(readmission_data$diag_3),"Circulatory", "Respiratory", "Digestive", "Diabetes", "Injury", "Musculoskeletal", "Genitourinary", "Neoplasms", "Other")

#set all entries for third primary diagnosis to "Other" as the default and only reassign those entries that belong to one of the other eight diagnosis categories
readmission_data$diag_3 <- "Other"

readmission_data$diag_3[((readmission_data$copy_diag_3 >= 390) & (readmission_data$copy_diag_3 <= 459)) | (readmission_data$copy_diag_3 == 785)] <- "Circulatory"

readmission_data$diag_3[((readmission_data$copy_diag_3 >= 460) & (readmission_data$copy_diag_3 <= 519)) | (readmission_data$copy_diag_3 == 786)] <- 'Respiratory'

readmission_data$diag_3[((readmission_data$copy_diag_3 >= 520) & (readmission_data$copy_diag_3 <= 579)) | (readmission_data$copy_diag_3 == 787)] <- 'Digestive'

readmission_data$diag_3[((readmission_data$copy_diag_3 >= 250) & (readmission_data$copy_diag_3 < 251))] <- 'Diabetes'

readmission_data$diag_3[((readmission_data$copy_diag_3 >= 800) & (readmission_data$copy_diag_3 <= 999))] <- 'Injury'

readmission_data$diag_3[((readmission_data$copy_diag_3 >= 710) & (readmission_data$copy_diag_3 <= 739))] <- 'Musculoskeletal'

readmission_data$diag_3[((readmission_data$copy_diag_3 >= 580) & (readmission_data$copy_diag_3 <= 629)) | (readmission_data$copy_diag_3 == 788)] <- 'Genitourinary'

readmission_data$diag_3[((readmission_data$copy_diag_3 >= 140) & (readmission_data$copy_diag_3 <= 239))] <- 'Neoplasm'

# drop temporary columns for diagnoses
readmission_data <- subset(readmission_data, select = -c(copy_diag_1, copy_diag_2, copy_diag_3))

# change diag attributes back into factors
readmission_data$diag_1 <- as.factor(readmission_data$diag_1)
readmission_data$diag_2 <- as.factor(readmission_data$diag_2)
readmission_data$diag_3 <- as.factor(readmission_data$diag_3)

# Rename values for unknown entries to 'Unknown' if applicable & create summaries for categorical variables

# race ... not well balanced (e.g. code Asian represents less than 1% of data) create levels: Caucasian and NotCaucasian
levels(readmission_data$race)[!(levels(readmission_data$race) == "Caucasian")] <- 'NotCaucasian'
summary (readmission_data$race)

# admission_type_id ... unbalanced data, so retain codes for 1 (Emergency), 2 (Urgent), and 3 (Elective) and relabel the rest as Uncommon (note: codes 5, 6, and 8 represent missing or unknown data)
readmission_data$admission_type_id <- as.factor(readmission_data$admission_type_id)
levels(readmission_data$admission_type_id)[levels(readmission_data$admission_type_id) %in% c('4', '5', '6', '7', '8')] <- 'Uncommon'
levels(readmission_data$admission_type_id)[levels(readmission_data$admission_type_id) == '1'] <- 'Emergency'
levels(readmission_data$admission_type_id)[levels(readmission_data$admission_type_id) == '2'] <- 'Urgent'
levels(readmission_data$admission_type_id)[levels(readmission_data$admission_type_id) == '3'] <- 'Elective'
summary (readmission_data$admission_type_id)

# discharge disposition id ... many levels and unbalanced ... combine all levels that represent less than 2% of the observations into an "Uncommon" category
readmission_data$discharge_disposition_id <- as.factor(readmission_data$discharge_disposition_id)
# discharge_disposition_id ... codes 18, 25, and 26 represent missing or unknown data
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) %in% c('18', '25', '26')] <- 'Unknown'
# discharge_disposition_id ... codes 12, 16, and 17 represent outpatient transfer / discharge /admission 
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) %in% c('12', '16', '17')] <- 'Outpatient'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '1'] <- 'Home'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '2'] <- 'Short term hospital'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '3'] <- 'SNF'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '4'] <- 'ICF'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '5'] <- 'Inpatient care institution'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '6'] <- 'Home health service'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '7'] <- 'Left AMA'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '8'] <- 'Home IV provider'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '9'] <- 'Inpatient to this hospital'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '10'] <- 'Neonatal aftercare'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '15'] <- 'Medicare swing bed'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '22'] <- 'Rehab'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '23'] <- 'Long term care'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '24'] <- 'Nursing facility'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '27'] <- 'Federal HCF'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '28'] <- 'Psychiatric hospital'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '29'] <- 'Critical Access Hospital'
levels(readmission_data$discharge_disposition_id)[levels(readmission_data$discharge_disposition_id) == '30'] <- 'Another HCI'
# reassign any levels with less than 2% of the data to "Uncommon"
# calculate the proportions for each level
props <- table(readmission_data$discharge_disposition_id) / length (readmission_data$discharge_disposition_id)
levels(readmission_data$discharge_disposition_id)[props < 0.02] <- 'Uncommon'
# remove any unused levels
readmission_data$discharge_disposition_id <- factor(readmission_data$discharge_disposition_id)
summary (readmission_data$discharge_disposition_id)

# admission source id ... many levels and unbalanced ... combine all levels that represent less than 2% of the observations into an "Uncommon" category
readmission_data$admission_source_id <- as.factor(readmission_data$admission_source_id)
# admission_source_id ... codes 9, 15, 17, 20, and 21 represent missing or unknown data
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) %in% c('9', '15', '17', '20', '21')] <- 'Unknown'

# codes 11, 12, 13, 14, 23, 24 refer to babies, childbirth, infants and will be releveled as "Babies"
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) %in% c('11', '12', '13', '14', '23', '24')] <- 'Babies'

# codes 4, 26 both mean transfer from hospital ... relevel as "Transfer from hospital"
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) %in% c('4', '26')] <- 'Transfer from hospital'

levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '1'] <- 'Physician Referral'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '2'] <- 'Clinic Referral'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '3'] <- 'HMO Referral'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '5'] <- 'Transfer from SNF'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '6'] <- 'Transfer from HCF'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '7'] <- 'ER'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '8'] <- 'Court'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '10'] <- 'Transfer from CAH'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '18'] <- 'Transfer From HHA'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '19'] <- 'Readmission to HHA'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '22'] <- 'Transfer from inpatients'
levels(readmission_data$admission_source_id)[levels(readmission_data$admission_source_id) == '25'] <- 'Transfer from ASC'
# reassign any levels with less than 2% of the data to "Uncommon"
# calculate the proportions for each level
props <- table(readmission_data$admission_source_id) / length (readmission_data$admission_source_id)
levels(readmission_data$admission_source_id)[props < 0.25] <- 'Uncommon'
# remove unused factors
readmission_data$admission_source_id <- factor(readmission_data$admission_source_id)
summary (readmission_data$admission_source_id)

# payer_code ... many levels and not well balanced (e.g. code FR only has 1 observation)
# create three levels: Unkown, MC (Medicaid), and Uncommon
levels(readmission_data$payer_code)[levels(readmission_data$payer_code) == "?"] <- 'Unknown'
levels(readmission_data$payer_code)[!(levels(readmission_data$payer_code) == "Unknown") & !(levels(readmission_data$payer_code) == "MC")] <- 'Uncommon'
summary (readmission_data$payer_code)

# medical specialty ... reassign any levels with less than 5% of the data to "Uncommon"
# calculate the proportions for each level
levels(readmission_data$medical_specialty)[levels(readmission_data$medical_specialty) == "?"] <- 'Unknown'
props <- table(readmission_data$medical_specialty) / length (readmission_data$medical_specialty)
levels(readmission_data$medical_specialty)[props < 0.05] <- 'Uncommon'
summary (readmission_data$medical_specialty)

# drop all medication attributes (due to data sparseness) except insulin
readmission_data <- subset(readmission_data, select = -c(24:40,42:46))

# drop patient_nbr and encounter_id since these are unique for each observation
readmission_data <- subset(readmission_data, select = -c(encounter_id, patient_nbr))

# gender
summary (readmission_data$gender)

# age ... choose the midpoint of the age range given and convert to a numeric variables
for (i in (0:9)) 
   {
    x <- paste("[", i*10, "-", (i+1)*10, ")", sep = "")
    mid_x <- as.character(5 + 10*i)
    levels(readmission_data$age)[levels(readmission_data$age) == x] <- mid_x
   }
readmission_data$age <- as.numeric(levels(readmission_data$age))[readmission_data$age]
summary (readmission_data$age)

#convert num_procedures, number_outpatient, number_emergency, and number_inpatient to categorical variables with values "None" and "At least one" since most are zeroes ... not normal
convert_function <- function(x){
   x[x > 0] <- 1
   x <- as.factor(x)
   levels(x)[levels(x)=="0"] <- "None"
   levels(x)[levels(x)=="1"] <- "At least one"
   return(x)
}

readmission_data$num_procedures <- convert_function(readmission_data$num_procedures)
readmission_data$number_outpatient <- convert_function(readmission_data$number_outpatient)
readmission_data$number_emergency <- convert_function(readmission_data$number_emergency)
readmission_data$number_inpatient <- convert_function(readmission_data$number_inpatient)

```

```{r}
# Basic summary statistics revisited after initial data cleaning and attribute selection
# Make a vector of categorical (factor) variables
a<- (sapply(readmission_data,class) == "factor")

# Categorical variables (use describe function of Hmisc package to display summary)
cat_vars <- readmission_data[, a]
Hmisc::describe(cat_vars)
for (cat_var in (1:ncol(cat_vars))){
  barplot(table(cat_vars[, cat_var]), main = names(cat_vars[cat_var]), las=2)
}

# Numerical variables (use stat.desc function of pastecs package to display summary)
num_vars <- readmission_data[, !a]
options(scipen=100)
options(digits=1)
stat.desc(num_vars)
summary(num_vars)
for (num_var in (1:ncol(num_vars))){
  hist(num_vars[num_var])
}

# Check skew and kurtosis for age, time_in_hospital, num_lab_procedures, and num_medications to determine normality
kurt <- kurtosis(num_vars$age)
skew <- skewness(num_vars$age)
print (paste("The distribution of the age attribute has skewness ", format(skew, digits = 2), " and kurtosis ", format(kurt, digits = 2, trim = FALSE, justify = "right"), "." , sep = ""))

kurt <- kurtosis(num_vars$time_in_hospital)
skew <- skewness(num_vars$time_in_hospital)
print (paste("The distribution of the time_in_hospital attribute has skewness ", format(skew, digits = 2), " and kurtosis ", format(kurt, digits = 2), "." , sep = ""))

kurt <- kurtosis(num_vars$num_lab_procedures)
skew <- skewness(num_vars$num_lab_procedures)
print (paste("The distribution of the num_lab_procedures attribute has skewness ", format(skew, digits = 2), " and kurtosis ", format(kurt, digits = 2), "." , sep = ""))

kurt <- kurtosis(num_vars$num_medications)
skew <- skewness(num_vars$num_medications)
print (paste("The distribution of the num_medications attribute has skewness ", format(skew, digits = 2), " and kurtosis ", format(kurt, digits = 2), "." , sep = ""))
```

```{r}
# Attempt to transform number of diagnoses attribute for normality
# Since dataset is too large to run Tukey's Ladder of Powers, choose a random sample from the dataset, perform the Tukey transformation and then apply the result to the entire dataset.
num_var_sample <- sample_n (num_vars, 5000)
tuk <- transformTukey(num_var_sample$number_diagnoses)
tuk2 <- transformTukey(tuk)

# strong evidence of non-normality even after Tukey transformation ... using binning instead (convert to categorical)
readmission_data$number_diagnoses[readmission_data$number_diagnoses <= 8] <- 0
readmission_data$number_diagnoses[readmission_data$number_diagnoses >= 9] <- 1
readmission_data$number_diagnoses <- as.factor(readmission_data$number_diagnoses)
levels(readmission_data$number_diagnoses)[levels(readmission_data$number_diagnoses)=="0"] <- "Low"
levels(readmission_data$number_diagnoses)[levels(readmission_data$number_diagnoses)=="1"] <- "High"

#include number_diagnoses in dataframe for categorical variables and remove from numerical variables dataframe
a<- (sapply(readmission_data,class) == "factor")
cat_vars <- readmission_data[, a]
num_vars <- readmission_data[, !a]
Hmisc::describe(cat_vars[15])
barplot(table(cat_vars[15]), main = names(cat_vars[15]), las=2)

```

```{r}
# Treatment of Outliers
# Boxplots of numerical attributes

num_obs <- nrow(num_vars)  # total number of records

# loop through all numeric variables to create a boxplot for each one
for (num_var in (1:ncol(num_vars))) {
  current_num_var <- num_vars[[num_var]]
  num_outliers <- length(boxplot.stats(current_num_var)$out)
  percent_outliers <- format(round(100 * num_outliers / num_obs, 2), nsmall = 2)
  boxplot(current_num_var, axes = FALSE, main=names(num_vars)[num_var], boxwex=0.2, xlab = paste("Number of outliers = ", num_outliers, " (", percent_outliers, "%)"))
  text(y = boxplot.stats(current_num_var)$stats, labels = boxplot.stats(current_num_var)$stats, x = 1.25)
  }

```

```{r}
#
# Check correlation between pairs of numeric attributes (use Spearman due to non-normality in some of attributes)
num_cor <- cor(num_vars, method = "spearman")
corrplot(num_cor, method="number", type="upper")
# no significant correlation between pairs of numeric attributes

# loop through pairs of categorical variables checking G-test for correlation significance (use 0.01 significance level)
num_cat_vars <- ncol(cat_vars)
num_num_vars <- ncol(num_vars)

for (i in 1:(num_cat_vars - 1)){
  for (j in (i + 1) : num_cat_vars) {
    gtest <- GTest(cat_vars[,i], cat_vars[,j])
    if (gtest$p.value <= 0.05) {
      print (paste(names(cat_vars[i]), " and ", names(cat_vars[j]), " are not independent.", format(gtest$p.value, digits = 4)))
    } else {
      print (paste(names(cat_vars[i]), " and ", names(cat_vars[j]), " are independent.", format(gtest$p.value, digits = 4)))
    }
  } 
}

# ANOVA to check independence of categorical and numerical attributes
for (i in (1 : num_num_vars)) {
  for (j in (1 : num_cat_vars)) {
    aov_result <- aov(num_vars[[i]] ~ cat_vars[[j]])
    aov_sum <- unlist(summary(aov_result))
    if (aov_sum["Pr(>F)1"] <= 0.05) {
      print (paste(names(num_vars[i]), " and ", names(cat_vars[j]), " are not independent. ", aov_sum["Pr(>F)1"]))
    } else {
      print (paste(names(num_vars[i]), " and ", names(cat_vars[j]), " are independent. ", aov_sum["Pr(>F)1"]))
    }
  }
}
```

```{r}
# Normalize the numerical attributes to assist support vector machine algorithm (since it uses a distance measure)
n <- !(sapply(readmission_data,class) == "factor") # numeric attributes
min_values <- sapply(readmission_data[, n], min)  # store minimum values for future "unscaling"
max_values <- sapply(readmission_data[, n], max)  # store maxmimum values for future "unscaling"
readmission_data[, n] <- sapply(readmission_data[, n], function(x) {(x - min(x)) / (max(x) - min(x))})

```

```{r}
# Create a categorical only dataset. Discretize continuous variables using interval = square root of the range and convert to factors. This is required to convert dataset to transactions for apriori arules algorithm
num_vars_disc <- num_vars
discretize <- function (df_column) {

  interval <- round (sqrt (max(df_column) - min(df_column)))
  df_column <- cut_interval(df_column, n = interval)
  
  return (df_column)
}

for (i in 1:ncol(num_vars)) {
  num_vars_disc[,i] <- as.factor(discretize (num_vars[,i]))
}

# create dataframe with only categorical data for entire readmission dataset
readmission_all_cat <- cbind(num_vars_disc, cat_vars)
```

```{r}
# use association rules (apriori algorithm) to induce knowledge about relationships between independent variables and class variable

#convert dataframe to transactions
readmission_trans <- as(readmission_all_cat, "transactions")
summary(readmission_trans)
itemLabels(readmission_trans)
# use lower support and confidence for readmitted<=30 because very few rules generated
rules_under30 <- apriori(readmission_trans, parameter = list(supp = 0.001, conf = 0.5, target = "rules"), appearance = list(rhs = "readmitted=<30"))
# use slightly higher support for readmitted>=30 to generate a reasonable number of rules
rules_over30 <- apriori(readmission_trans, parameter = list(supp = 0.015, conf = 0.5, target = "rules"), appearance = list(rhs = "readmitted=>30"))
# use higher confidence for readmitted=NO to generate a reasonable number of rules
rules_NOT <- apriori(readmission_trans, parameter = list(supp = 0.002, conf = 0.89, target = "rules"), appearance = list(rhs = "readmitted=NO"))

inspect(head(rules_under30, by = "lift")) # view rules for patients readmitted in under 30 days
inspect(head(rules_over30, by = "lift")) # view rules for patients readmitted in over 30 days
inspect(head(rules_NOT, by = "lift")) # view rules for patients not readmitted
```

```{r}
# combine readmitted<=30 and readmitted>=30 to create a binary class categorical dataset
readmission_binary_cat <- readmission_all_cat
readmission_binary_cat$readmitted <- factor(ifelse(readmission_binary_cat$readmitted == "NO","NO","YES"))
```

```{r}
# Using the binary class dataset, determine association rules for simply readmitted or not
readmission_trans_binary <- as(readmission_binary_cat, "transactions")
summary(readmission_trans_binary)
itemLabels(readmission_trans_binary)
# generate rules for "readmitted=YES"
rules__binary_YES <- apriori(readmission_trans_binary, parameter = list(supp = 0.04, conf = 0.6, target = "rules"), appearance = list(rhs = "readmitted=YES"))
# use higher confidence for readmitted=NO to generate a reasonable number of rules
rules_binary_NO <- apriori(readmission_trans_binary, parameter = list(supp = 0.004, conf = 0.85, target = "rules"), appearance = list(rhs = "readmitted=NO"))

inspect(head(rules__binary_YES, by = "lift")) # view rules for patients readmitted in under 30 days
inspect(head(rules_binary_NO, by = "lift")) # view rules for patients readmitted in over 30 days
```

```{r}
# Use FSelector and LVQ to identify and remove attributes that are not significantly contributing as predictors of the target variable "readmitted".

#use sample of approximately 10% of data because algorithm takes too long with full dataset running LVQ
set.seed(136)
readmission_sample <- readmission_all_cat[sample(1:nrow(readmission_all_cat), 7000), ]

# use FSelector information gain to determine most important attributes based on information gain (print those values that are higher than the median)
att.scores <- information.gain(readmitted ~ ., readmission_sample)
att.scores.sig <- att.scores[att.scores$attr_importance > median(att.scores$attr_importance),,drop = FALSE]
top_F <- att.scores.sig[order(-att.scores.sig),,drop = FALSE]

# Use caret package and learning vector quantization (LVQ) to determine the most important attributes
# prepare training scheme
control <- trainControl(method="repeatedcv", number=3, repeats=2)
# train the model
lvq_model <- train(readmitted ~ ., data=readmission_sample, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(lvq_model, scale=FALSE)
plot(importance)
# create dataframe with the averages of importance for all three classes and keep only those above median
top_L <- importance$importance
top_L$average <- rowMeans(importance$importance)
top_L <- subset(top_L, select = c("average"))
importance.median <- median(top_L$average)
top_L <- top_L[top_L$average > importance.median,, drop = FALSE]
top_L <- top_L[order(-top_L),, drop = FALSE]

# create list of attributes above median for both FSelector and LVQ
top_F.names <- rownames(top_F)
top_L.names <- rownames(top_L)
for (i in 1:length(top_F.names)) {
  if (top_F.names[i] %in% top_L.names) {print(top_F.names[i])}
}

```

```{r}
#Split the data into training (80%) and test set (20%). Consider possibly treatment for imbalance in the target variable, readmitted.

#Partition the dataset into training and test sets (use the discretized dataframe)
set.seed(136)
trainIndex <- createDataPartition(readmission_data$readmitted, p = .8, 
                                  list = FALSE, 
                                  times = 1)

readmitted_train <- readmission_data[trainIndex,] 
readmitted_test <- readmission_data[-trainIndex,] 

```

```{r}
# Apply various classification techniques (Naïve Bayes, decision tree, random forest, and support vector machines) to the data. First, use all attributes. Second, use only common attributes found in FSelector information gain and LVQ importance feature selection techniques from above.

# *****
# 
# Function to generate model using cross validation and create confusion matrix.
# Parameters passed to function are ml_type (NB for Naive Bayes, DT for decision tree, RF for random forest, and SV for support vector machines), num_folds (number of folds in cross validation), and readmitted_df (readmission data dataframe)
# The function will create a total confusion matrix for the particular model being implemented by summing the confusion matrices of each of fold in the cross validation
#
generate_conf_matrix <- function (ml_type, num_folds, readmitted_df) {

#Partition the dataset into training and test sets for cross-validation
  set.seed(136)
  trainIndex <- createFolds(readmitted_df$readmitted, k = num_folds, list = TRUE)
  conf_matrix <- matrix(data = 0, nrow = nlevels(readmitted_df$readmitted), ncol = nlevels(readmitted_df$readmitted)) #initialize confusion matrix

#Loop through all folds of cross validation, compute confusion matrix, keep running total
  for (i in 1:num_folds) {
    
    r_train <- readmitted_df[-trainIndex[[i]],] # use all folds except current index for training
    r_test <- readmitted_df[trainIndex[[i]],]   # use current index fold for test

    if (ml_type == "NB") {model1 <- naiveBayes(readmitted ~ ., data = r_train, laplace = 1)} 
    else if (ml_type == "DT") {model1 <- J48(readmitted ~ ., data = r_train)}
    else if (ml_type == "RF") {model1 <- randomForest(readmitted ~ ., data=r_train, ntree=100)}
    else if (ml_type == "SV") {model1 <- svm(readmitted ~ ., data = r_train, kernel = 'linear', gamma = 0.05, cost = 5)}
    
    # predict classes based on trained model for test data    
    preds <- predict(model1, newdata = r_test)
    # generate confusion matrix    
    new_conf_matrix <- table(preds, r_test$readmitted)
    # keep running total of conf_matrix to facilitate calculation of mean after loop finishes
    conf_matrix <- conf_matrix + new_conf_matrix
    }

  # label and return confusion matrix
  names(dimnames(conf_matrix)) <- c("Predicted Class", "Actual Class")
  return(conf_matrix)
  }
```

```{r}
#calculate the accuracy and F1 score for a confusion matrix (cm). If binary class then use standard method for calculating accuracy and F1 score. If multiclass then use micro (one vs all) approach since class distribution is not even (note that for multiclass confusion matrix, micro precision, recall, and F1 are equal)
calc_perf_meas <- function (cm) {
  n_instances = sum(cm) # number of instances
  n_classes = ncol(cm) # number of classes
  n_correct = diag(cm) # number of correctly classified instances per class 
  rowsums = apply(cm, 1, sum) # number of predictions per class
  colsums = apply(cm, 2, sum) # number of instances per class
  tp <- cm[1,1]
  fp <- cm[1,2]
  fn <- cm[2,1]

  if (n_classes == 2) {
      acc <- sum(n_correct) / n_instances                # accuracy
      prec <- tp / (tp + fp)                        # precision
      recall <- tp / (tp + fn)                      # recall
      f1 <- 2 * prec * recall / (prec + recall)     # F1 score
  } 
  else if (n_classes > 2) {
#one versus all matrices for each of the 3 class (<30, >30, NO) ... assumes each class in turn is positive
      oneVsAll = lapply(1 : n_classes,    
                      function(i){
                        v = c(cm[i,i],
                              rowsums[i] - cm[i,i],
                              colsums[i] - cm[i,i],
                              n_instances - rowsums[i] - colsums[i] + cm[i,i]);
                        return(matrix(v, nrow = 2, byrow = TRUE))})
      sum_cm = matrix(0, nrow = 2, ncol = 2)
      for(i in 1 : n_classes){sum_cm = sum_cm + oneVsAll[[i]]}  # add up confusion matrices for classes
   
      acc <- sum(diag(sum_cm)) / sum(sum_cm)  # average micro accuracy
      f1 <- sum_cm[1,1] / sum(sum_cm[1,])      #find the micro average F1 score = precision = recall 
  }
  perf_measures <- list ("Accuracy" = acc, "F1 Score" = f1)
  return(perf_measures)
}
```

```{r}
# Function to run models (uses cross-validation on the training data sent to the function). Parameters passed to function are the dataframe to be used and codes for which models to be run (NB, DT, RF, SV). Use tictoc function to record runtime.

run_models <- function (df, models_to_use) {

  num_folds <- 4  # set number of folds for cross-validation
  perf_measures_NB <- list ("Accuracy" = 0, "F1 Score" = 0) # initialize performance measures lists
  perf_measures_DT <- list ("Accuracy" = 0, "F1 Score" = 0)
  perf_measures_RF <- list ("Accuracy" = 0, "F1 Score" = 0)
  perf_measures_SV <- list ("Accuracy" = 0, "F1 Score" = 0)

    if ("NB" %in% models_to_use) {
#
# Naive Bayes
#
      tic("Naive Bayes - All attributes")
      conf_matrix <- generate_conf_matrix ("NB", num_folds, df)
      toc()
      conf_matrix
      perf_measures_NB <- format(calc_perf_meas (conf_matrix), digits = 3)
      perf_measures_NB
    }

    if ("DT" %in% models_to_use) {
#
# Decision Tree
#
      tic("Decision Tree - All attributes")
      conf_matrix <- generate_conf_matrix ("DT", num_folds, df)
      toc()
      conf_matrix
      perf_measures_DT <- format(calc_perf_meas (conf_matrix), digits = 3)
      perf_measures_DT
    }

    if ("RF" %in% models_to_use) {
#
# Random forest
#
      tic("Random Forest - All attributes")
      conf_matrix <- generate_conf_matrix ("RF", num_folds, df)
      toc()
      conf_matrix
      perf_measures_RF <- format(calc_perf_meas (conf_matrix), digits = 3)
      perf_measures_RF
    }
  
    if ("SV" %in% models_to_use) {  
#
# Support Vector Machine
#
      tic("Support Vector Machine - All attributes")
      conf_matrix <- generate_conf_matrix ("SV", num_folds, df)
      toc()
      conf_matrix
      perf_measures_SV <- format(calc_perf_meas (conf_matrix), digits = 3)
      perf_measures_SV
    }
  perf_measures <- list (NB = perf_measures_NB, DT = perf_measures_DT, RF = perf_measures_RF, SV = perf_measures_SV)
  return(perf_measures)
}
```

```{r}
#
# Run the models using all attributes.
#
perf_measures <- run_models(readmitted_train, c("NB", "DT", "RF")) # not using SV because it takes too long
perf_measures

```

```{r}
#
# Run the models using attributes determined from feature selection: number_inpatient, discharge_disposition_id, age, number_diagnoses, num_lab_procedures, time_in_hospital, diag_2, diabetesMed 
reduced_attrib_vector <- c('number_inpatient', 'discharge_disposition_id', 'age', 'number_diagnoses', 'num_lab_procedures', 'time_in_hospital', 'diag_2', 'diabetesMed', 'payer_code', 'number_outpatient', 'readmitted')
reduced_df <- readmitted_train[, reduced_attrib_vector]

perf_measures <- run_models(reduced_df, c("NB", "DT", "RF", "SV"))
perf_measures
```
```{r}
# combine readmitted<=30 and readmitted>=30 to create a binary class dataset
readmission_binary_class <- readmission_data
readmission_binary_class$readmitted <- factor(ifelse(readmission_binary_class$readmitted == "NO","NO","YES"))
t<-table(readmission_binary_class$readmitted)
p<- t/sum(t)
t
p
```

```{r}
#
# Run the models using a binary class attribute for readmitted (readmitted or not)
#
# partition the readmission data with binary class attribute into training and test sets using same index created previously for multiclass data
readmitted_binary_train <- readmission_binary_class[trainIndex,] 
readmitted_binary_test <- readmission_binary_class[-trainIndex,] 
#
# Run the models on the full attribute dataset (uses cross validation on the training set):
#
perf_measures <- run_models(readmitted_binary_train, c("NB", "DT", "RF"))
perf_measures
#
# Run the models using attributes determined from feature selection (uses cross validation on the training set):
#
reduced_binary_df <- readmitted_binary_train[, reduced_attrib_vector]

perf_measures <- run_models(reduced_binary_df, c("NB", "DT", "RF"))
perf_measures
```

```{r}
#
#	Experiment with various numbers of folds in cross-validation to find the optimal number for the training set (graph performance measures against number of cross-validation folds to determine where number of folds stabilizes)
max_folds <- 10
# set up plot
plot(1, type="n", xlab="Number of Folds", ylab="Performance Meaures", xlim=c(0, 10), ylim=c(0.5, 0.8))
title(main = "Naive Bayes: Accuracy & F1 Scores vs Number of CV Folds")
legend(0, 0.4, legend=c("Accuracy", "F1 Score"),col=c("red", "blue"))
  #
  # Naive Bayes on reduced set
  #
for (nf in 2:max_folds) {
  conf_matrix <- generate_conf_matrix ("NB", nf, reduced_df)
  perf_measures <- format(calc_perf_meas (conf_matrix), digits = 3)
  points(x = nf, y = perf_measures[1], col = "red", pch = 15)   # plot accuracy for current number of folds
  points(x = nf, y = perf_measures[2], col = "blue", pch = 19)   # plot F1 Score for current number of folds
}
  #
  # Decision tree on reduced set
  #
# set up plot
plot(1, type="n", xlab="Number of Folds", ylab="Performance Meaures", xlim=c(0, 10), ylim=c(0.5, 0.8))
title(main = "Decision Tree: Accuracy & F1 Scores vs Number of CV Folds")
legend(0, 0.4, legend=c("Accuracy", "F1 Score"),col=c("red", "blue"))

for (nf in 2:max_folds) {
  conf_matrix <- generate_conf_matrix ("DT", nf, reduced_df)
  perf_measures <- format(calc_perf_meas (conf_matrix), digits = 3)
  points(x = nf, y = perf_measures[1], col = "red", pch = 15)   # plot accuracy for current number of folds
  points(x = nf, y = perf_measures[2], col = "blue", pch = 19)   # plot F1 Score for current number of folds
}
```

```{r}
# Balance the multiclass training dataset by undersampling the majority classes and rerun models to see if that improves accuracy and F1 scores.
# Use the number of observations from the smallest class (readmitted <30) as number randomly selected from other classes
n_smallest_class <- length(readmitted_train$readmitted[readmitted_train$readmitted=="<30"])
readmitted_balanced <- as.data.frame(readmitted_train %>% group_by(readmitted) %>% sample_n(n_smallest_class))
#
# Run the models using the balanced dataset on the full attribute set (uses cross-validation)
perf_measures <- run_models(readmitted_balanced, c("NB", "DT", "RF"))
perf_measures
#
# Run the models using the balanced dataset on the reduced attribute set (uses cross-validation)
reduced_balanced <- readmitted_balanced[, reduced_attrib_vector]
perf_measures <- run_models(reduced_balanced, c("NB", "DT", "RF"))
perf_measures
```

```{r}
# Convert to binary class (combine >30 and NOT classes ... i.e. either readmitted under 30 days or not at all).
readmission_binary_class <- readmitted_train
readmission_binary_class$readmitted <- factor(ifelse(readmission_binary_class$readmitted == "<30", "YES", "NO"))
readmission_binary_class$readmitted <- relevel(readmission_binary_class$readmitted, "YES")
table(readmission_binary_class$readmitted)
```

```{r}
# Balance the binary class training dataset by undersampling the majority class and then run models.
# Use the number of observations from the smallest class (readmitted <30) as number randomly selected from other class
n_smallest_class <- length(readmission_binary_class$readmitted[readmission_binary_class$readmitted == "YES"])
readmitted_balanced1 <- as.data.frame(readmission_binary_class %>% group_by(readmitted) %>% sample_n(n_smallest_class))
#
# Run the models using the balanced dataset on the full attribute set (uses cross-validation)
perf_measures <- run_models(readmitted_balanced1, c("NB", "DT", "RF"))
perf_measures
#
# Run the models using the balanced dataset on the reduced attribute set (uses cross-validation)
reduced_balanced1 <- readmitted_balanced1[, reduced_attrib_vector]
perf_measures <- run_models(reduced_balanced1, c("NB", "DT", "RF"))
perf_measures
```

```{r}
# Balance the binary class training dataset using SMOTE (oversample minority class) and then run models.
#
# Use SMOTE to balance the classes
t <- table(readmission_binary_class$readmitted) # table with number in each class
o_balance_factor <- round(100 * (max(t[1], t[2]) - min(t[1], t[2])) / (min(t[1], t[2]))) # factor for perc.over in smote algorithm to create enough observations to balance dataset
u_balance_factor <- round( 100 * max(t[1], t[2]) / (max(t[1], t[2]) - min(t[1], t[2]))) # factor for perc.under in smote algorithm to keep the majority class with the same number of observations as before
readmitted_balanced2 <- SMOTE(readmitted ~ ., readmission_binary_class, perc.over = o_balance_factor, perc.under = u_balance_factor)
# Run models (Naive Bayes, decision tree, random forests) using cross-validation on balanced dataset with all attributes
perf_measures <- run_models(readmitted_balanced2, c("NB", "DT", "RF"))
perf_measures
#
# Run models (Naive Bayes, decision tree, random forests) using cross-validation on balanced dataset with reduced attribute set
reduced_balanced2 <- readmitted_balanced2[, reduced_attrib_vector]
perf_measures <- run_models(reduced_balanced2, c("NB", "DT", "RF"))
perf_measures
```

```{r}
# Generalize the results of the classification using boosting ensemble technique.
#
# function to create boost ensemble for given dataframe
boost_fun <- function (dframe, iters) {
  
  # split dataframe into training and test set
  set.seed(136)
  ti <- createDataPartition(dframe$readmitted, p = .8, list = FALSE, times = 1)

  btrain <- dframe[ti,] # training set
  btest <- dframe[-ti,] # testing set

  model_adaboost <- adaboost(readmitted ~ ., btrain, iters) # run adaboost ensemble algorithm
  pred <- predict(model_adaboost, newdata=btest) # predict values for test set
  pred$class <- relevel(pred$class, "YES") # make the "YES" class appear first
  conf_mat <- table(pred$class,btest$readmitted) # create confusion matrix
  perf_measures <- calc_perf_meas(conf_mat)
  return(perf_measures)
  }
```

```{r}
# optimize iterations for boost ensemble (use reduced SMOTE dataset)
max_iters <- 10
# set up plot
plot(1, type="n", xlab="Number of Folds", ylab="Performance Meaures", xlim=c(0, 10), ylim=c(0, 1))
title(main = "Boost: Accuracy & F1 Scores vs Number of Iterations")
legend(0, 0.4, legend=c("Accuracy", "F1 Score"),col=c("red", "blue"))

for (ni in 1:max_iters) {
  tic(paste("number of iterations", ni))
  perf_measures <- boost_fun(reduced_balanced2, ni)
  toc()
  points(x = ni, y = perf_measures[1], col = "red", pch = 15)   # plot acc. for current number of iterations
  points(x = ni, y = perf_measures[2], col = "blue", pch = 19)   # plot F1 for current number of iterations
}
```

```{r}
# boost ensemble for binary class data, unbalanced, full attribute set
tic("Boost Ensemble - Unbalanced, binary, all attributes")
perf_measures <- boost_fun(readmission_binary_class, 7)
toc()
format(perf_measures, digits = 3)

# boost ensemble for binary class data, unbalanced, reduced attribute set
tic("Boost Ensemble - Unbalanced, binary, reduced attribute set")
perf_measures <- boost_fun(readmission_binary_class[, reduced_attrib_vector], 7)
toc()
format(perf_measures, digits = 3)

# boost ensemble for balanced (undersampling majority) binary class data full attribute set
tic("Boost Ensemble - Balanced (undersampling), binary, all attributes")
perf_measures <- boost_fun(readmitted_balanced1, 7)
toc()
format(perf_measures, digits = 3)

# boost ensemble for balanced (undersampling majority) binary class data reduced attribute set
tic("Boost Ensemble - Balanced (undersampling), binary, reduced attribute set")
perf_measures <- boost_fun(reduced_balanced1, 7)
toc()
format(perf_measures, digits = 3)

# boost ensemble for balanced (synthetic oversampling minority) binary class data full attribute set
tic("Boost Ensemble - Balanced (SMOTE), binary, all attributes")
perf_measures <- boost_fun(readmitted_balanced2, 7)
toc()
format(perf_measures, digits = 3)

# boost ensemble for balanced (synthetic oversampling minority) binary class data reduced attribute set
tic("Boost Ensemble - Balanced (SMOTE), binary, reduced attribute set")
perf_measures <- boost_fun(reduced_balanced2, 7)
toc()
format(perf_measures, digits = 3)
```

```{r}
# Use the test dataset to test the four best models
#
# Random forest on multiclass reduced attribute dataset
reduced_test <- readmitted_test[, reduced_attrib_vector]
tic("Random forest on multiclass full attribute dataset")
model1 <- randomForest(readmitted ~ ., data=readmitted_train[, reduced_attrib_vector], ntree=100)
preds1 <- predict(model1, newdata = reduced_test)
toc()
conf_matrix1 <- table(preds1, reduced_test$readmitted)
names(dimnames(conf_matrix1)) <- c("Predicted Class", "Actual Class")
perf_measures <- calc_perf_meas(conf_matrix1)
format(perf_measures, digits = 3)
#
# Create a binary class version of test dataset
readmitted_binary_test <- readmitted_test
readmitted_binary_test$readmitted <- factor(ifelse(readmitted_binary_test$readmitted == "<30", "YES", "NO"))
readmitted_binary_test$readmitted <- relevel(readmitted_binary_test$readmitted, "YES")
#
# Random forest on binary, SMOTE balanced, full attribute dataset
tic("Random forest on binary, SMOTE balanced full attribute dataset")
model2 <- randomForest(readmitted ~ ., data=readmitted_balanced2, ntree=100)
preds2 <- predict(model2, newdata = readmitted_binary_test)
toc()
conf_matrix2 <- table(preds2, readmitted_binary_test$readmitted)
names(dimnames(conf_matrix2)) <- c("Predicted Class", "Actual Class")
perf_measures <- calc_perf_meas(conf_matrix2)
format(perf_measures, digits = 3)
#
# Create a reduced attribute binary class version of test dataset
reduced_binary_test <- readmitted_binary_test[, reduced_attrib_vector]
#
# Decision tree on binary, SMOTE balanced, reduced attribute dataset
tic("Decision tree on binary, SMOTE balanced reduced attribute dataset")
model3 <- J48(readmitted ~ ., data=readmitted_balanced2[, reduced_attrib_vector])
preds3 <- predict(model3, newdata = reduced_binary_test)
toc()
conf_matrix3 <- table(preds3, reduced_binary_test$readmitted)
names(dimnames(conf_matrix3)) <- c("Predicted Class", "Actual Class")
perf_measures <- calc_perf_meas(conf_matrix3)
format(perf_measures, digits = 3)
#
# Adaboost on binary, SMOTE balanced, full attribute dataset
tic("Adaboost on binary, SMOTE balanced full attribute dataset")
model4 <- adaboost(readmitted ~ ., data=readmitted_balanced2, 7)
preds4 <- predict(model4, newdata = readmitted_binary_test)
toc()
preds4$class <- relevel(preds4$class, "YES") # make the "YES" class appear first
conf_matrix4 <- table(preds4$class, readmitted_binary_test$readmitted)
names(dimnames(conf_matrix4)) <- c("Predicted Class", "Actual Class")
perf_measures <- calc_perf_meas(conf_matrix4)
format(perf_measures, digits = 3)


#
# 

```
